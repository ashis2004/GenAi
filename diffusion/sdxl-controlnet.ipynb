{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2887207",
   "metadata": {},
   "source": [
    "# SDXL ControlNet - Architecture-Guided Image Generation\n",
    "\n",
    "This notebook demonstrates how to use ControlNet with Stable Diffusion XL (SDXL) for precise control over image generation. We'll explore various ControlNet models and show how to generate images from architectural sketches, poses, depth maps, and more.\n",
    "\n",
    "## What is ControlNet?\n",
    "ControlNet is a neural network structure that allows you to control diffusion models by adding extra conditions. It enables:\n",
    "- Pose control using OpenPose\n",
    "- Edge control using Canny edge detection\n",
    "- Depth control using depth maps\n",
    "- Scribble control for sketch-to-image\n",
    "- And many more conditioning types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80321759",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe04780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q diffusers transformers accelerate controlnet-aux opencv-python pillow matplotlib torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dadc258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionXLControlNetPipeline, \n",
    "    ControlNetModel,\n",
    "    AutoencoderKL,\n",
    "    StableDiffusionXLImg2ImgPipeline\n",
    ")\n",
    "from diffusers.utils import load_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from controlnet_aux import (\n",
    "    CannyDetector, \n",
    "    OpenposeDetector, \n",
    "    MidasDetector,\n",
    "    HEDdetector,\n",
    "    LineartDetector\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f0b71",
   "metadata": {},
   "source": [
    "## 2. Setup ControlNet Models and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc52dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize different ControlNet models\n",
    "def setup_controlnet_models():\n",
    "    \"\"\"Setup various ControlNet models for different conditioning types.\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Canny ControlNet for edge control\n",
    "    print(\"Loading Canny ControlNet...\")\n",
    "    models['canny'] = ControlNetModel.from_pretrained(\n",
    "        \"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    \n",
    "    # Depth ControlNet for depth control\n",
    "    print(\"Loading Depth ControlNet...\")\n",
    "    models['depth'] = ControlNetModel.from_pretrained(\n",
    "        \"diffusers/controlnet-depth-sdxl-1.0\", \n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    \n",
    "    return models\n",
    "\n",
    "controlnet_models = setup_controlnet_models()\n",
    "print(\"ControlNet models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d53072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SDXL pipeline with ControlNet\n",
    "def create_controlnet_pipeline(controlnet_model):\n",
    "    \"\"\"Create SDXL ControlNet pipeline.\"\"\"\n",
    "    \n",
    "    # Load VAE for better quality\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        \"madebyollin/sdxl-vae-fp16-fix\", \n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        controlnet=controlnet_model,\n",
    "        vae=vae,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\"\n",
    "    )\n",
    "    \n",
    "    # Enable memory efficient attention\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "print(\"Pipeline creation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d16bbb",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad809e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessors\n",
    "canny_detector = CannyDetector()\n",
    "depth_detector = MidasDetector.from_pretrained('valhalla/t2iadapter-aux-models')\n",
    "openpose_detector = OpenposeDetector.from_pretrained('lllyasviel/Annotators')\n",
    "hed_detector = HEDdetector.from_pretrained('lllyasviel/Annotators')\n",
    "lineart_detector = LineartDetector.from_pretrained('lllyasviel/Annotators')\n",
    "\n",
    "print(\"Preprocessors initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, processor_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Preprocess image based on the ControlNet type.\n",
    "    \n",
    "    Args:\n",
    "        image: Input PIL Image\n",
    "        processor_type: Type of preprocessing ('canny', 'depth', 'openpose', etc.)\n",
    "        **kwargs: Additional parameters for processors\n",
    "    \n",
    "    Returns:\n",
    "        Processed PIL Image\n",
    "    \"\"\"\n",
    "    \n",
    "    if processor_type == 'canny':\n",
    "        low_threshold = kwargs.get('low_threshold', 100)\n",
    "        high_threshold = kwargs.get('high_threshold', 200)\n",
    "        return canny_detector(image, low_threshold=low_threshold, high_threshold=high_threshold)\n",
    "    \n",
    "    elif processor_type == 'depth':\n",
    "        return depth_detector(image)\n",
    "    \n",
    "    elif processor_type == 'openpose':\n",
    "        return openpose_detector(image)\n",
    "    \n",
    "    elif processor_type == 'hed':\n",
    "        return hed_detector(image)\n",
    "    \n",
    "    elif processor_type == 'lineart':\n",
    "        return lineart_detector(image)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown processor type: {processor_type}\")\n",
    "\n",
    "print(\"Preprocessing function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79e11e",
   "metadata": {},
   "source": [
    "## 4. Utility Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec735e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, titles, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display multiple images side by side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=figsize)\n",
    "    \n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for img, title, ax in zip(images, titles, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_sample_architectural_sketch():\n",
    "    \"\"\"\n",
    "    Create a simple architectural sketch for demonstration.\n",
    "    \"\"\"\n",
    "    # Create a simple house sketch\n",
    "    img = Image.new('RGB', (512, 512), color='white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # House outline\n",
    "    draw.rectangle([100, 300, 400, 450], outline='black', width=3)\n",
    "    \n",
    "    # Roof\n",
    "    draw.polygon([80, 300, 250, 200, 420, 300], outline='black', width=3)\n",
    "    \n",
    "    # Door\n",
    "    draw.rectangle([220, 350, 280, 450], outline='black', width=2)\n",
    "    \n",
    "    # Windows\n",
    "    draw.rectangle([130, 320, 180, 360], outline='black', width=2)\n",
    "    draw.rectangle([320, 320, 370, 360], outline='black', width=2)\n",
    "    \n",
    "    # Window crosses\n",
    "    draw.line([155, 320, 155, 360], fill='black', width=1)\n",
    "    draw.line([130, 340, 180, 340], fill='black', width=1)\n",
    "    draw.line([345, 320, 345, 360], fill='black', width=1)\n",
    "    draw.line([320, 340, 370, 340], fill='black', width=1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "print(\"Utility functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2ae68",
   "metadata": {},
   "source": [
    "## 5. Canny Edge ControlNet Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede14d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load an architectural sketch\n",
    "sketch_image = create_sample_architectural_sketch()\n",
    "\n",
    "# Process with Canny edge detection\n",
    "canny_image = preprocess_image(sketch_image, 'canny', low_threshold=50, high_threshold=150)\n",
    "\n",
    "# Display original and processed images\n",
    "display_images(\n",
    "    [sketch_image, canny_image], \n",
    "    ['Original Sketch', 'Canny Edges'],\n",
    "    figsize=(10, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image using Canny ControlNet\n",
    "def generate_with_canny_control(canny_image, prompt, negative_prompt=\"\", num_images=1):\n",
    "    \"\"\"\n",
    "    Generate images using Canny ControlNet.\n",
    "    \"\"\"\n",
    "    # Create pipeline\n",
    "    pipe = create_controlnet_pipeline(controlnet_models['canny'])\n",
    "    \n",
    "    # Generate images\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=canny_image,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=1.0,\n",
    "        num_images_per_prompt=num_images,\n",
    "        height=512,\n",
    "        width=512\n",
    "    ).images\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Test prompts for architectural generation\n",
    "prompts = [\n",
    "    \"modern luxury house, architectural photography, beautiful lighting, detailed, 4k\",\n",
    "    \"victorian style house, ornate details, vintage architecture, professional photo\",\n",
    "    \"futuristic house, glass and steel, minimalist design, contemporary architecture\"\n",
    "]\n",
    "\n",
    "print(\"Canny ControlNet generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with different prompts\n",
    "negative_prompt = \"blurry, low quality, distorted, ugly, bad architecture\"\n",
    "\n",
    "print(\"Generating images with Canny ControlNet...\")\n",
    "for i, prompt in enumerate(prompts[:2]):  # Generate for first 2 prompts\n",
    "    print(f\"\\nGenerating: {prompt}\")\n",
    "    \n",
    "    generated_images = generate_with_canny_control(\n",
    "        canny_image, \n",
    "        prompt, \n",
    "        negative_prompt\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    display_images(\n",
    "        [canny_image, generated_images[0]], \n",
    "        ['Canny Control', f'Generated: Style {i+1}'],\n",
    "        figsize=(10, 5)\n",
    "    )\n",
    "\n",
    "print(\"Canny ControlNet examples completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022b6d6",
   "metadata": {},
   "source": [
    "## 6. Depth ControlNet Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For depth control, let's create a simple depth map or use an existing image\n",
    "def create_depth_map_example():\n",
    "    \"\"\"Create a simple depth map for demonstration.\"\"\"\n",
    "    # Create a gradient depth map (closer = darker, further = lighter)\n",
    "    img = Image.new('L', (512, 512), color=128)\n",
    "    \n",
    "    # Convert to numpy for easier manipulation\n",
    "    depth_array = np.array(img)\n",
    "    \n",
    "    # Create depth gradient (simulate a room with perspective)\n",
    "    for y in range(512):\n",
    "        for x in range(512):\n",
    "            # Distance from center bottom\n",
    "            dist = np.sqrt((x - 256)**2 + (y - 400)**2)\n",
    "            depth_value = min(255, max(0, int(255 - (dist / 3))))\n",
    "            depth_array[y, x] = depth_value\n",
    "    \n",
    "    return Image.fromarray(depth_array).convert('RGB')\n",
    "\n",
    "# Create depth map\n",
    "depth_map = create_depth_map_example()\n",
    "\n",
    "# Display depth map\n",
    "display_images([depth_map], ['Depth Map Example'], figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with depth control\n",
    "def generate_with_depth_control(depth_image, prompt, negative_prompt=\"\"):\n",
    "    \"\"\"\n",
    "    Generate images using Depth ControlNet.\n",
    "    \"\"\"\n",
    "    # Create pipeline\n",
    "    pipe = create_controlnet_pipeline(controlnet_models['depth'])\n",
    "    \n",
    "    # Generate image\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=depth_image,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=0.8,\n",
    "        height=512,\n",
    "        width=512\n",
    "    ).images\n",
    "    \n",
    "    return images[0]\n",
    "\n",
    "# Test depth-controlled generation\n",
    "depth_prompt = \"luxurious interior room, modern furniture, beautiful lighting, architectural photography\"\n",
    "depth_negative = \"cluttered, dark, low quality, distorted\"\n",
    "\n",
    "print(\"Generating with depth control...\")\n",
    "depth_generated = generate_with_depth_control(depth_map, depth_prompt, depth_negative)\n",
    "\n",
    "# Display results\n",
    "display_images(\n",
    "    [depth_map, depth_generated], \n",
    "    ['Depth Control', 'Generated Interior'],\n",
    "    figsize=(10, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e98ebd",
   "metadata": {},
   "source": [
    "## 7. Advanced ControlNet Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e90a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_controlnet_generation(control_image1, control_image2, prompt, controlnet_types, conditioning_scales):\n",
    "    \"\"\"\n",
    "    Generate images using multiple ControlNets simultaneously.\n",
    "    \n",
    "    Args:\n",
    "        control_image1, control_image2: Control images\n",
    "        prompt: Text prompt\n",
    "        controlnet_types: List of ControlNet types to use\n",
    "        conditioning_scales: List of conditioning scales for each ControlNet\n",
    "    \"\"\"\n",
    "    from diffusers import MultiControlNetModel\n",
    "    \n",
    "    # Create multi-ControlNet\n",
    "    controlnets = [controlnet_models[ct] for ct in controlnet_types]\n",
    "    multi_controlnet = MultiControlNetModel(controlnets)\n",
    "    \n",
    "    # Create pipeline with multi-ControlNet\n",
    "    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        controlnet=multi_controlnet,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    \n",
    "    pipe.enable_model_cpu_offload()\n",
    "    \n",
    "    # Generate with multiple controls\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        image=[control_image1, control_image2],\n",
    "        num_inference_steps=25,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=conditioning_scales,\n",
    "        height=512,\n",
    "        width=512\n",
    "    ).images\n",
    "    \n",
    "    return images[0]\n",
    "\n",
    "print(\"Multi-ControlNet function ready (advanced technique)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b1871",
   "metadata": {},
   "source": [
    "## 8. ControlNet with Custom Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5318457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_controlnet_strength(control_image, prompt, strength_values):\n",
    "    \"\"\"\n",
    "    Demonstrate how ControlNet conditioning strength affects generation.\n",
    "    \"\"\"\n",
    "    pipe = create_controlnet_pipeline(controlnet_models['canny'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for strength in strength_values:\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            image=control_image,\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=7.5,\n",
    "            controlnet_conditioning_scale=strength,\n",
    "            height=512,\n",
    "            width=512\n",
    "        ).images[0]\n",
    "        \n",
    "        results.append(image)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different conditioning strengths\n",
    "strength_values = [0.5, 1.0, 1.5]\n",
    "test_prompt = \"beautiful modern house, architectural photography, professional\"\n",
    "\n",
    "print(\"Testing different ControlNet conditioning strengths...\")\n",
    "strength_results = adjust_controlnet_strength(canny_image, test_prompt, strength_values)\n",
    "\n",
    "# Display results\n",
    "titles = [f'Strength: {s}' for s in strength_values]\n",
    "display_images(strength_results, titles, figsize=(15, 5))\n",
    "\n",
    "print(\"ControlNet strength comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711aa9b",
   "metadata": {},
   "source": [
    "## 9. Image-to-Image with ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d12bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlnet_img2img(base_image, control_image, prompt, strength=0.8):\n",
    "    \"\"\"\n",
    "    Combine ControlNet with image-to-image generation.\n",
    "    \"\"\"\n",
    "    # Create ControlNet pipeline\n",
    "    controlnet_pipe = create_controlnet_pipeline(controlnet_models['canny'])\n",
    "    \n",
    "    # First pass: Generate with ControlNet\n",
    "    controlnet_result = controlnet_pipe(\n",
    "        prompt=prompt,\n",
    "        image=control_image,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5,\n",
    "        controlnet_conditioning_scale=1.0,\n",
    "    ).images[0]\n",
    "    \n",
    "    # Second pass: Refine with img2img\n",
    "    img2img_pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    img2img_pipe.enable_model_cpu_offload()\n",
    "    \n",
    "    refined_result = img2img_pipe(\n",
    "        prompt=prompt,\n",
    "        image=controlnet_result,\n",
    "        strength=strength,\n",
    "        num_inference_steps=20,\n",
    "        guidance_scale=7.5\n",
    "    ).images[0]\n",
    "    \n",
    "    return controlnet_result, refined_result\n",
    "\n",
    "print(\"ControlNet + Img2Img function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fcd26",
   "metadata": {},
   "source": [
    "## 10. Batch Processing and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_style_transfer(control_image, style_prompts):\n",
    "    \"\"\"\n",
    "    Generate multiple architectural styles from the same control image.\n",
    "    \"\"\"\n",
    "    pipe = create_controlnet_pipeline(controlnet_models['canny'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in style_prompts:\n",
    "        print(f\"Generating: {prompt[:50]}...\")\n",
    "        \n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=\"low quality, blurry, distorted\",\n",
    "            image=control_image,\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=7.5,\n",
    "            controlnet_conditioning_scale=1.0,\n",
    "        ).images[0]\n",
    "        \n",
    "        results.append(image)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define architectural styles\n",
    "architectural_styles = [\n",
    "    \"modern minimalist house, clean lines, glass and concrete, contemporary architecture\",\n",
    "    \"traditional japanese house, wooden structure, zen garden, cultural architecture\",\n",
    "    \"art deco mansion, ornate details, 1920s style, luxury architecture\",\n",
    "    \"sustainable eco house, green roof, solar panels, environmental architecture\"\n",
    "]\n",
    "\n",
    "print(\"Generating multiple architectural styles...\")\n",
    "style_results = batch_style_transfer(canny_image, architectural_styles[:2])  # Generate 2 styles\n",
    "\n",
    "# Display results\n",
    "display_images(\n",
    "    [canny_image] + style_results, \n",
    "    ['Control Image', 'Modern Style', 'Japanese Style'],\n",
    "    figsize=(15, 5)\n",
    ")\n",
    "\n",
    "print(\"Batch style transfer completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6f7b4",
   "metadata": {},
   "source": [
    "## 11. Quality Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_controlnet_adherence(control_image, generated_image, control_type='canny'):\n",
    "    \"\"\"\n",
    "    Evaluate how well the generated image adheres to the ControlNet conditioning.\n",
    "    \"\"\"\n",
    "    # Extract features from both images\n",
    "    if control_type == 'canny':\n",
    "        control_processed = preprocess_image(control_image, 'canny')\n",
    "        generated_processed = preprocess_image(generated_image, 'canny')\n",
    "    elif control_type == 'depth':\n",
    "        control_processed = preprocess_image(control_image, 'depth')\n",
    "        generated_processed = preprocess_image(generated_image, 'depth')\n",
    "    \n",
    "    # Convert to numpy for comparison\n",
    "    control_array = np.array(control_processed.convert('L'))\n",
    "    generated_array = np.array(generated_processed.convert('L'))\n",
    "    \n",
    "    # Calculate similarity metrics\n",
    "    # Structural similarity\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    similarity_score = ssim(control_array, generated_array)\n",
    "    \n",
    "    # Edge preservation (for canny)\n",
    "    if control_type == 'canny':\n",
    "        edge_preservation = np.mean(np.abs(control_array - generated_array)) / 255.0\n",
    "        edge_preservation = 1.0 - edge_preservation  # Convert to similarity score\n",
    "    else:\n",
    "        edge_preservation = similarity_score\n",
    "    \n",
    "    return {\n",
    "        'structural_similarity': similarity_score,\n",
    "        'edge_preservation': edge_preservation,\n",
    "        'overall_score': (similarity_score + edge_preservation) / 2\n",
    "    }\n",
    "\n",
    "# Evaluate our generated images\n",
    "if 'generated_images' in locals() and len(generated_images) > 0:\n",
    "    evaluation = evaluate_controlnet_adherence(canny_image, generated_images[0], 'canny')\n",
    "    \n",
    "    print(\"ControlNet Adherence Evaluation:\")\n",
    "    print(f\"  Structural Similarity: {evaluation['structural_similarity']:.3f}\")\n",
    "    print(f\"  Edge Preservation: {evaluation['edge_preservation']:.3f}\")\n",
    "    print(f\"  Overall Score: {evaluation['overall_score']:.3f}\")\n",
    "else:\n",
    "    print(\"No generated images available for evaluation\")\n",
    "\n",
    "print(\"\\nEvaluation function ready for use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bb32b",
   "metadata": {},
   "source": [
    "## 12. Optimization and Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization tips\n",
    "optimization_tips = \"\"\"\n",
    "=== SDXL CONTROLNET OPTIMIZATION TIPS ===\n",
    "\n",
    "1. MEMORY OPTIMIZATION:\n",
    "   - Use enable_model_cpu_offload() to move models between CPU/GPU\n",
    "   - Enable enable_xformers_memory_efficient_attention() for lower VRAM usage\n",
    "   - Use torch.float16 precision to reduce memory footprint\n",
    "   - Clear CUDA cache between generations: torch.cuda.empty_cache()\n",
    "\n",
    "2. SPEED OPTIMIZATION:\n",
    "   - Reduce num_inference_steps (20-30 is usually sufficient)\n",
    "   - Use smaller image dimensions during experimentation\n",
    "   - Batch multiple generations when possible\n",
    "   - Consider using DPM++ schedulers for faster sampling\n",
    "\n",
    "3. QUALITY OPTIMIZATION:\n",
    "   - Fine-tune guidance_scale (7.0-9.0 works well for SDXL)\n",
    "   - Adjust controlnet_conditioning_scale (0.8-1.2 range)\n",
    "   - Use high-quality control images with clear features\n",
    "   - Experiment with different negative prompts\n",
    "\n",
    "4. CONTROLNET SPECIFIC:\n",
    "   - Canny: Adjust thresholds for edge detection sensitivity\n",
    "   - Depth: Ensure proper depth map contrast\n",
    "   - OpenPose: Use clear, unambiguous poses\n",
    "   - Multiple ControlNets: Balance conditioning scales\n",
    "\n",
    "5. PRODUCTION CONSIDERATIONS:\n",
    "   - Implement proper error handling and retries\n",
    "   - Monitor GPU memory usage and implement limits\n",
    "   - Cache models to avoid repeated loading\n",
    "   - Use proper async/await for web applications\n",
    "\"\"\"\n",
    "\n",
    "print(optimization_tips)\n",
    "\n",
    "# Memory cleanup function\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory after generation.\"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "# Clean up\n",
    "cleanup_memory()\n",
    "print(\"\\nMemory cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe1e04",
   "metadata": {},
   "source": [
    "## 13. Save and Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generation_results(images, prompts, control_images, output_dir=\"./controlnet_outputs\"):\n",
    "    \"\"\"\n",
    "    Save generation results with metadata.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (image, prompt, control_img) in enumerate(zip(images, prompts, control_images)):\n",
    "        # Save generated image\n",
    "        gen_filename = f\"{timestamp}_generated_{i:03d}.png\"\n",
    "        image.save(os.path.join(output_dir, gen_filename))\n",
    "        \n",
    "        # Save control image\n",
    "        ctrl_filename = f\"{timestamp}_control_{i:03d}.png\"\n",
    "        control_img.save(os.path.join(output_dir, ctrl_filename))\n",
    "        \n",
    "        # Record metadata\n",
    "        results.append({\n",
    "            \"generated_image\": gen_filename,\n",
    "            \"control_image\": ctrl_filename,\n",
    "            \"prompt\": prompt,\n",
    "            \"timestamp\": timestamp\n",
    "        })\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_filename = f\"{timestamp}_metadata.json\"\n",
    "    with open(os.path.join(output_dir, metadata_filename), 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_dir}\")\n",
    "    return results\n",
    "\n",
    "print(\"Save function ready\")\n",
    "\n",
    "# Example of saving results (uncomment to use)\n",
    "# if 'style_results' in locals():\n",
    "#     save_generation_results(\n",
    "#         style_results[:2], \n",
    "#         architectural_styles[:2], \n",
    "#         [canny_image] * 2\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fbe4b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated comprehensive usage of SDXL ControlNet for architecture-guided image generation:\n",
    "\n",
    "### Key Achievements:\n",
    "1. **ControlNet Setup** - Configured multiple ControlNet models (Canny, Depth)\n",
    "2. **Preprocessing Pipeline** - Implemented various image preprocessing techniques\n",
    "3. **Architectural Generation** - Generated buildings from sketches and control inputs\n",
    "4. **Style Transfer** - Applied different architectural styles to the same structure\n",
    "5. **Quality Evaluation** - Measured ControlNet adherence and generation quality\n",
    "6. **Optimization** - Implemented memory and performance optimizations\n",
    "\n",
    "### ControlNet Applications:\n",
    "- **Architecture Visualization**: Convert sketches to photorealistic buildings\n",
    "- **Interior Design**: Control room layouts with depth maps\n",
    "- **Concept Art**: Generate variations while maintaining structural elements\n",
    "- **Urban Planning**: Visualize building designs in context\n",
    "\n",
    "### Best Practices Learned:\n",
    "1. **Control Image Quality**: High-quality, clear control images produce better results\n",
    "2. **Conditioning Scale**: Balance between prompt following and control adherence\n",
    "3. **Prompt Engineering**: Specific, detailed prompts yield better architectural features\n",
    "4. **Memory Management**: Essential for running SDXL on consumer hardware\n",
    "\n",
    "### Next Steps:\n",
    "1. **Custom ControlNet Training**: Train ControlNets on architectural datasets\n",
    "2. **Multi-Modal Control**: Combine different ControlNet types\n",
    "3. **Real-time Applications**: Build interactive architectural design tools\n",
    "4. **3D Integration**: Combine with 3D rendering pipelines\n",
    "5. **Production Deployment**: Scale for architectural firms and design studios\n",
    "\n",
    "**Note**: This notebook provides a foundation for architectural AI generation. For production use, consider fine-tuning models on architectural datasets and implementing proper user interfaces."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
