{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee57108",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning with Mistral 7B\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Mistral 7B model using LoRA (Low-Rank Adaptation) for a specific task. We'll customize the model for a domain-specific use case while keeping memory usage manageable.\n",
    "\n",
    "## What is LoRA?\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that:\n",
    "- Reduces trainable parameters by 99%\n",
    "- Maintains model performance\n",
    "- Enables fine-tuning large models on consumer GPUs\n",
    "- Allows for quick task switching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a249e",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers peft datasets accelerate bitsandbytes wandb torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c2048",
   "metadata": {},
   "source": [
    "## 2. Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "new_model = \"mistral-7b-custom-lora\"\n",
    "\n",
    "# QLoRA configuration for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbe47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb42f3",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Data\n",
    "\n",
    "For this example, we'll create a custom dataset for fine-tuning. In practice, you would use your domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4944ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample training data (replace with your domain-specific data)\n",
    "training_data = [\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of machine learning\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to analyze data, identify patterns, and make predictions or decisions.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the main types of machine learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The three main types of machine learning are: 1) Supervised learning - learns from labeled data, 2) Unsupervised learning - finds patterns in unlabeled data, and 3) Reinforcement learning - learns through trial and error with rewards and penalties.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe neural networks\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Neural networks are computational models inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers that process information. They excel at pattern recognition and are fundamental to deep learning.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What is deep learning?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Deep learning is a subset of machine learning that uses neural networks with multiple hidden layers to model and understand complex patterns in data. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain overfitting in machine learning\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Overfitting occurs when a model learns the training data too well, including noise and random fluctuations, resulting in poor performance on new, unseen data. It can be prevented through techniques like regularization, cross-validation, and using more training data.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame and then to Dataset\n",
    "df = pd.DataFrame(training_data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(f\"Training dataset size: {len(dataset)}\")\n",
    "print(\"Sample data point:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for training\n",
    "def format_instruction(sample):\n",
    "    \"\"\"\n",
    "    Format the training data into a conversational format.\n",
    "    \"\"\"\n",
    "    if sample['input']:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply formatting\n",
    "def preprocess_dataset(dataset):\n",
    "    return dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
    "\n",
    "formatted_dataset = preprocess_dataset(dataset)\n",
    "\n",
    "print(\"Formatted sample:\")\n",
    "print(formatted_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193bfe6",
   "metadata": {},
   "source": [
    "## 4. LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of adaptation\n",
    "    lora_alpha=32,  # LoRA scaling parameter\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],  # Modules to apply LoRA to\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # LoRA dropout\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"LoRA configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee10148",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=None,  # Set to \"wandb\" if you want to use Weights & Biases\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    do_eval=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228720e9",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66570382",
   "metadata": {},
   "source": [
    "## 7. Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ac480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation before training\n",
    "def generate_response(model, tokenizer, prompt, max_length=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"### Instruction:\\nExplain what artificial intelligence is\\n\\n### Response:\\n\"\n",
    "\n",
    "print(\"=== BEFORE TRAINING ===\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "pre_training_response = generate_response(model, tokenizer, test_prompt)\n",
    "print(f\"Response: {pre_training_response}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7601bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "print(\"This may take several minutes depending on your hardware.\")\n",
    "\n",
    "# Clear cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce8650",
   "metadata": {},
   "source": [
    "## 8. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "\n",
    "print(f\"Model saved to {new_model}\")\n",
    "\n",
    "# Save training logs\n",
    "import json\n",
    "training_logs = trainer.state.log_history\n",
    "with open(f\"{new_model}/training_logs.json\", \"w\") as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "\n",
    "print(\"Training logs saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4755e",
   "metadata": {},
   "source": [
    "## 9. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation after training\n",
    "print(\"=== AFTER TRAINING ===\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "post_training_response = generate_response(model, tokenizer, test_prompt)\n",
    "print(f\"Response: {post_training_response}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Compare responses\n",
    "print(\"=== COMPARISON ===\")\n",
    "print(\"BEFORE TRAINING:\")\n",
    "print(pre_training_response)\n",
    "print(\"\\nAFTER TRAINING:\")\n",
    "print(post_training_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce081f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple prompts\n",
    "test_prompts = [\n",
    "    \"### Instruction:\\nWhat is supervised learning?\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nExplain gradient descent\\n\\n### Response:\\n\",\n",
    "    \"### Instruction:\\nWhat are the applications of AI?\\n\\n### Response:\\n\"\n",
    "]\n",
    "\n",
    "print(\"=== TESTING MULTIPLE PROMPTS ===\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(f\"Prompt: {prompt.split('Response:')[0]}Response:\")\n",
    "    response = generate_response(model, tokenizer, prompt, max_length=150)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e239b",
   "metadata": {},
   "source": [
    "## 10. Load and Use the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how to load the fine-tuned model later\n",
    "def load_fine_tuned_model(base_model_name, adapter_path):\n",
    "    \"\"\"\n",
    "    Load a fine-tuned LoRA model.\n",
    "    \"\"\"\n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "print(\"Function to load fine-tuned model defined.\")\n",
    "print(\"\\nTo load the model later, use:\")\n",
    "print(f\"model, tokenizer = load_fine_tuned_model('{model_name}', '{new_model}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10fc474",
   "metadata": {},
   "source": [
    "## 11. Model Analysis and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "def analyze_model_performance():\n",
    "    \"\"\"\n",
    "    Analyze the performance of the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Get model size information\n",
    "    base_params = sum(p.numel() for p in model.get_base_model().parameters())\n",
    "    lora_params = sum(p.numel() for n, p in model.named_parameters() if 'lora' in n)\n",
    "    \n",
    "    print(\"=== MODEL ANALYSIS ===\")\n",
    "    print(f\"Base model parameters: {base_params:,}\")\n",
    "    print(f\"LoRA parameters: {lora_params:,}\")\n",
    "    print(f\"LoRA overhead: {(lora_params / base_params) * 100:.4f}%\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"\\nGPU Memory Usage:\")\n",
    "        print(f\"  Allocated: {memory_allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {memory_reserved:.2f} GB\")\n",
    "    \n",
    "    # Training efficiency\n",
    "    if training_logs:\n",
    "        final_loss = training_logs[-1].get('train_loss', 'N/A')\n",
    "        print(f\"\\nTraining Metrics:\")\n",
    "        print(f\"  Final training loss: {final_loss}\")\n",
    "        print(f\"  Training steps: {len(training_logs)}\")\n",
    "\n",
    "analyze_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3114c9",
   "metadata": {},
   "source": [
    "## 12. Best Practices and Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices for LoRA fine-tuning\n",
    "best_practices = \"\"\"\n",
    "=== LORA FINE-TUNING BEST PRACTICES ===\n",
    "\n",
    "1. HYPERPARAMETER TUNING:\n",
    "   - Start with r=16, alpha=32 for most tasks\n",
    "   - Increase r for more complex tasks (up to 64)\n",
    "   - Keep alpha = 2 * r as a general rule\n",
    "   - Use lower learning rates (1e-4 to 5e-4)\n",
    "\n",
    "2. TARGET MODULES:\n",
    "   - Include attention modules (q_proj, k_proj, v_proj, o_proj)\n",
    "   - Add feed-forward modules for better performance\n",
    "   - Include lm_head for generation tasks\n",
    "\n",
    "3. DATA PREPARATION:\n",
    "   - Use consistent formatting across all examples\n",
    "   - Include clear instruction-response pairs\n",
    "   - Ensure data quality over quantity\n",
    "   - Use appropriate sequence lengths\n",
    "\n",
    "4. TRAINING STRATEGIES:\n",
    "   - Start with small datasets to test\n",
    "   - Use gradient accumulation for larger effective batch sizes\n",
    "   - Monitor training loss and stop if overfitting\n",
    "   - Save checkpoints regularly\n",
    "\n",
    "5. EVALUATION:\n",
    "   - Test on held-out data\n",
    "   - Compare against base model performance\n",
    "   - Use domain-specific evaluation metrics\n",
    "   - Consider human evaluation for quality\n",
    "\n",
    "6. DEPLOYMENT:\n",
    "   - LoRA adapters are small (typically < 100MB)\n",
    "   - Can switch between different LoRA adapters quickly\n",
    "   - Merge adapters for faster inference if needed\n",
    "   - Test thoroughly before production deployment\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780b640",
   "metadata": {},
   "source": [
    "## 13. Cleanup and Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77639d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"GPU memory cleared\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"Current GPU memory usage: {memory_allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194148c0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Setting up LoRA fine-tuning** with Mistral 7B using 4-bit quantization\n",
    "2. **Preparing training data** in instruction-response format\n",
    "3. **Configuring LoRA parameters** for efficient training\n",
    "4. **Training the model** with minimal resource usage\n",
    "5. **Evaluating improvements** through before/after comparisons\n",
    "6. **Saving and loading** the fine-tuned model\n",
    "\n",
    "### Key Benefits of LoRA:\n",
    "- **Memory Efficient**: Trains only 0.1-1% of original parameters\n",
    "- **Fast Training**: Significantly reduced training time\n",
    "- **Modular**: Easy to switch between different adaptations\n",
    "- **Cost-Effective**: Can run on consumer GPUs\n",
    "\n",
    "### Next Steps:\n",
    "1. **Scale up** with larger, domain-specific datasets\n",
    "2. **Experiment** with different LoRA configurations\n",
    "3. **Evaluate** on downstream tasks\n",
    "4. **Deploy** in production environments\n",
    "5. **Combine** multiple LoRA adapters for multi-task models\n",
    "\n",
    "**Note**: This is a demonstration with a small dataset. For production use, ensure you have high-quality, diverse training data and proper evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
