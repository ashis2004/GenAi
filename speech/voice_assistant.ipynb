{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae6deaa",
   "metadata": {},
   "source": [
    "# Voice Assistant with Speech-to-Text and LLM Integration\n",
    "\n",
    "This notebook demonstrates how to build a complete voice assistant that:\n",
    "1. Listens to speech input using Whisper\n",
    "2. Processes the text with a Large Language Model\n",
    "3. Responds with synthesized speech using Bark\n",
    "4. Provides a web interface using Streamlit\n",
    "\n",
    "## Features\n",
    "- Real-time speech recognition\n",
    "- Multi-language support\n",
    "- Contextual conversations\n",
    "- Natural voice synthesis\n",
    "- Web-based interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a72bb",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b732d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q whisper-openai bark transformers torch torchaudio sounddevice soundfile streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24815a21",
   "metadata": {},
   "source": [
    "## 2. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf02af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceAssistant:\n",
    "    def __init__(self):\n",
    "        self.is_listening = False\n",
    "        self.conversation_history = []\n",
    "        self.audio_queue = queue.Queue()\n",
    "        \n",
    "        print(\"Loading Whisper model...\")\n",
    "        self.whisper_model = whisper.load_model(\"base\")\n",
    "        \n",
    "        print(\"Loading LLM...\")\n",
    "        self.llm_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"microsoft/DialoGPT-medium\",\n",
    "            tokenizer=\"microsoft/DialoGPT-medium\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        print(\"Loading Bark TTS models...\")\n",
    "        preload_models()\n",
    "        \n",
    "        print(\"Voice Assistant initialized!\")\n",
    "    \n",
    "    def record_audio(self, duration=5, sample_rate=16000):\n",
    "        \"\"\"Record audio from microphone.\"\"\"\n",
    "        print(f\"Recording for {duration} seconds...\")\n",
    "        audio_data = sd.rec(int(duration * sample_rate), \n",
    "                           samplerate=sample_rate, \n",
    "                           channels=1, \n",
    "                           dtype=np.float32)\n",
    "        sd.wait()\n",
    "        return audio_data.flatten()\n",
    "    \n",
    "    def transcribe_audio(self, audio_data, sample_rate=16000):\n",
    "        \"\"\"Transcribe audio to text using Whisper.\"\"\"\n",
    "        # Save temporary audio file\n",
    "        temp_file = \"temp_audio.wav\"\n",
    "        sf.write(temp_file, audio_data, sample_rate)\n",
    "        \n",
    "        # Transcribe\n",
    "        result = self.whisper_model.transcribe(temp_file)\n",
    "        return result[\"text\"].strip()\n",
    "    \n",
    "    def generate_response(self, user_input):\n",
    "        \"\"\"Generate response using LLM.\"\"\"\n",
    "        # Add context from conversation history\n",
    "        context = \"\\n\".join(self.conversation_history[-3:])  # Last 3 exchanges\n",
    "        if context:\n",
    "            prompt = f\"{context}\\nUser: {user_input}\\nAssistant:\"\n",
    "        else:\n",
    "            prompt = f\"User: {user_input}\\nAssistant:\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.llm_pipeline(\n",
    "            prompt,\n",
    "            max_length=len(prompt) + 100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.llm_pipeline.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Extract assistant response\n",
    "        full_response = response[0]['generated_text']\n",
    "        assistant_response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        self.conversation_history.append(f\"Assistant: {assistant_response}\")\n",
    "        \n",
    "        return assistant_response\n",
    "    \n",
    "    def text_to_speech(self, text, voice_preset=\"v2/en_speaker_6\"):\n",
    "        \"\"\"Convert text to speech using Bark.\"\"\"\n",
    "        print(f\"Generating speech: {text[:50]}...\")\n",
    "        audio_array = generate_audio(text, history_prompt=voice_preset)\n",
    "        return audio_array\n",
    "    \n",
    "    def play_audio(self, audio_array):\n",
    "        \"\"\"Play audio array.\"\"\"\n",
    "        sd.play(audio_array, SAMPLE_RATE)\n",
    "        sd.wait()\n",
    "    \n",
    "    def process_voice_input(self, duration=5):\n",
    "        \"\"\"Complete voice processing pipeline.\"\"\"\n",
    "        try:\n",
    "            # Record audio\n",
    "            audio_data = self.record_audio(duration)\n",
    "            \n",
    "            # Transcribe to text\n",
    "            user_text = self.transcribe_audio(audio_data)\n",
    "            print(f\"You said: {user_text}\")\n",
    "            \n",
    "            if not user_text.strip():\n",
    "                print(\"No speech detected\")\n",
    "                return None, None\n",
    "            \n",
    "            # Generate response\n",
    "            response_text = self.generate_response(user_text)\n",
    "            print(f\"Assistant: {response_text}\")\n",
    "            \n",
    "            # Convert to speech\n",
    "            response_audio = self.text_to_speech(response_text)\n",
    "            \n",
    "            # Play response\n",
    "            self.play_audio(response_audio)\n",
    "            \n",
    "            return user_text, response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in voice processing: {e}\")\n",
    "            return None, None\n",
    "\n",
    "# Initialize the voice assistant\n",
    "assistant = VoiceAssistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391e82c",
   "metadata": {},
   "source": [
    "## 3. Test Individual Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a11e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test speech-to-text\n",
    "print(\"Testing Speech-to-Text...\")\n",
    "print(\"Say something when prompted!\")\n",
    "\n",
    "# Uncomment to test with actual microphone input\n",
    "# audio = assistant.record_audio(duration=3)\n",
    "# transcription = assistant.transcribe_audio(audio)\n",
    "# print(f\"Transcribed: {transcription}\")\n",
    "\n",
    "# For demo purposes, simulate with text\n",
    "test_input = \"Hello, how are you today?\"\n",
    "print(f\"Simulated input: {test_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLM response generation\n",
    "print(\"Testing LLM Response Generation...\")\n",
    "response = assistant.generate_response(test_input)\n",
    "print(f\"Generated response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3640fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text-to-speech\n",
    "print(\"Testing Text-to-Speech...\")\n",
    "speech_audio = assistant.text_to_speech(response)\n",
    "print(f\"Generated speech audio with shape: {speech_audio.shape}\")\n",
    "\n",
    "# Play the audio (uncomment to hear)\n",
    "# assistant.play_audio(speech_audio)\n",
    "print(\"TTS test completed (audio playback commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56ba41",
   "metadata": {},
   "source": [
    "## 4. Interactive Voice Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf289db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_voice_session(num_interactions=3):\n",
    "    \"\"\"Run an interactive voice session.\"\"\"\n",
    "    print(\"\\n=== Interactive Voice Assistant Session ===\")\n",
    "    print(\"Note: Microphone and audio playback are disabled in this demo\")\n",
    "    print(\"Simulating voice interactions...\\n\")\n",
    "    \n",
    "    # Simulate conversation with text inputs\n",
    "    simulated_inputs = [\n",
    "        \"What's the weather like?\",\n",
    "        \"Tell me a joke\",\n",
    "        \"What can you help me with?\"\n",
    "    ]\n",
    "    \n",
    "    for i, simulated_input in enumerate(simulated_inputs[:num_interactions]):\n",
    "        print(f\"--- Interaction {i+1} ---\")\n",
    "        print(f\"Simulated user input: {simulated_input}\")\n",
    "        \n",
    "        # Generate and display response\n",
    "        response = assistant.generate_response(simulated_input)\n",
    "        print(f\"Assistant response: {response}\")\n",
    "        \n",
    "        # Simulate TTS (without actual audio playback)\n",
    "        print(\"[Speech synthesis completed]\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Session completed!\")\n",
    "    \n",
    "    # Display conversation history\n",
    "    print(\"\\n=== Conversation History ===\")\n",
    "    for entry in assistant.conversation_history:\n",
    "        print(entry)\n",
    "\n",
    "# Run interactive session\n",
    "interactive_voice_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e5ea7",
   "metadata": {},
   "source": [
    "## 5. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedVoiceAssistant(VoiceAssistant):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wake_word = \"assistant\"\n",
    "        self.is_wake_word_active = False\n",
    "        self.user_preferences = {\n",
    "            \"voice\": \"v2/en_speaker_6\",\n",
    "            \"language\": \"en\",\n",
    "            \"response_length\": \"medium\"\n",
    "        }\n",
    "    \n",
    "    def detect_wake_word(self, text):\n",
    "        \"\"\"Detect wake word in transcribed text.\"\"\"\n",
    "        return self.wake_word.lower() in text.lower()\n",
    "    \n",
    "    def extract_intent(self, text):\n",
    "        \"\"\"Extract user intent from text.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if any(word in text_lower for word in ['weather', 'temperature', 'forecast']):\n",
    "            return 'weather'\n",
    "        elif any(word in text_lower for word in ['joke', 'funny', 'laugh']):\n",
    "            return 'joke'\n",
    "        elif any(word in text_lower for word in ['time', 'clock', 'hour']):\n",
    "            return 'time'\n",
    "        elif any(word in text_lower for word in ['help', 'assist', 'support']):\n",
    "            return 'help'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def generate_intent_based_response(self, user_input, intent):\n",
    "        \"\"\"Generate response based on detected intent.\"\"\"\n",
    "        if intent == 'weather':\n",
    "            return \"I don't have access to real-time weather data, but I'd recommend checking a weather app or website for current conditions.\"\n",
    "        elif intent == 'joke':\n",
    "            jokes = [\n",
    "                \"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "                \"What do you call a fake noodle? An impasta!\",\n",
    "                \"Why did the scarecrow win an award? He was outstanding in his field!\"\n",
    "            ]\n",
    "            import random\n",
    "            return random.choice(jokes)\n",
    "        elif intent == 'time':\n",
    "            from datetime import datetime\n",
    "            current_time = datetime.now().strftime(\"%I:%M %p\")\n",
    "            return f\"The current time is {current_time}.\"\n",
    "        elif intent == 'help':\n",
    "            return \"I'm a voice assistant that can help with general questions, tell jokes, provide the time, and have conversations. What would you like to know?\"\n",
    "        else:\n",
    "            return self.generate_response(user_input)\n",
    "    \n",
    "    def process_advanced_voice_input(self, duration=5):\n",
    "        \"\"\"Advanced voice processing with intent detection.\"\"\"\n",
    "        # Simulate for demo\n",
    "        simulated_inputs = [\n",
    "            \"Assistant, what's the weather like?\",\n",
    "            \"Tell me a joke please\",\n",
    "            \"What time is it?\"\n",
    "        ]\n",
    "        \n",
    "        for user_text in simulated_inputs:\n",
    "            print(f\"\\nProcessing: {user_text}\")\n",
    "            \n",
    "            # Check for wake word\n",
    "            if self.detect_wake_word(user_text):\n",
    "                print(\"Wake word detected!\")\n",
    "                \n",
    "                # Extract intent\n",
    "                intent = self.extract_intent(user_text)\n",
    "                print(f\"Detected intent: {intent}\")\n",
    "                \n",
    "                # Generate response based on intent\n",
    "                response = self.generate_intent_based_response(user_text, intent)\n",
    "                print(f\"Response: {response}\")\n",
    "                \n",
    "                # Simulate TTS\n",
    "                print(\"[Speech synthesis completed]\")\n",
    "            else:\n",
    "                print(\"Wake word not detected, ignoring...\")\n",
    "\n",
    "# Test advanced features\n",
    "advanced_assistant = AdvancedVoiceAssistant()\n",
    "advanced_assistant.process_advanced_voice_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db6d60",
   "metadata": {},
   "source": [
    "## 6. Streamlit Web Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit app file\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "import whisper\n",
    "from bark import generate_audio, SAMPLE_RATE\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    \"\"\"Load and cache models.\"\"\"\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"microsoft/DialoGPT-medium\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    return whisper_model, llm_pipeline\n",
    "\n",
    "def main():\n",
    "    st.title(\"🎤 Voice Assistant with GenAI\")\n",
    "    st.write(\"A complete voice assistant powered by Whisper, LLM, and Bark\")\n",
    "    \n",
    "    # Load models\n",
    "    with st.spinner(\"Loading AI models...\"):\n",
    "        whisper_model, llm_pipeline = load_models()\n",
    "    \n",
    "    # Initialize session state\n",
    "    if \"conversation\" not in st.session_state:\n",
    "        st.session_state.conversation = []\n",
    "    \n",
    "    # Text input option\n",
    "    st.subheader(\"💬 Text Conversation\")\n",
    "    user_input = st.text_input(\"Type your message:\")\n",
    "    \n",
    "    if st.button(\"Send Message\"):\n",
    "        if user_input:\n",
    "            # Generate response\n",
    "            with st.spinner(\"Generating response...\"):\n",
    "                response = llm_pipeline(\n",
    "                    f\"User: {user_input}\\\\nAssistant:\",\n",
    "                    max_length=100,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )[0]['generated_text']\n",
    "                \n",
    "                assistant_response = response.split(\"Assistant:\")[-1].strip()\n",
    "            \n",
    "            # Add to conversation\n",
    "            st.session_state.conversation.append((\"You\", user_input))\n",
    "            st.session_state.conversation.append((\"Assistant\", assistant_response))\n",
    "            \n",
    "            # Generate and provide audio\n",
    "            with st.spinner(\"Generating speech...\"):\n",
    "                audio_array = generate_audio(assistant_response)\n",
    "                \n",
    "                # Convert to audio file\n",
    "                audio_buffer = io.BytesIO()\n",
    "                sf.write(audio_buffer, audio_array, SAMPLE_RATE, format='WAV')\n",
    "                audio_buffer.seek(0)\n",
    "                \n",
    "                st.audio(audio_buffer.read(), format='audio/wav')\n",
    "    \n",
    "    # Display conversation history\n",
    "    if st.session_state.conversation:\n",
    "        st.subheader(\"📝 Conversation History\")\n",
    "        for speaker, message in st.session_state.conversation:\n",
    "            if speaker == \"You\":\n",
    "                st.write(f\"**{speaker}:** {message}\")\n",
    "            else:\n",
    "                st.write(f\"*{speaker}:* {message}\")\n",
    "    \n",
    "    # Audio upload option\n",
    "    st.subheader(\"🎵 Audio Upload\")\n",
    "    uploaded_audio = st.file_uploader(\n",
    "        \"Upload an audio file\", \n",
    "        type=['wav', 'mp3', 'm4a']\n",
    "    )\n",
    "    \n",
    "    if uploaded_audio:\n",
    "        if st.button(\"Process Audio\"):\n",
    "            with st.spinner(\"Transcribing audio...\"):\n",
    "                # Save uploaded file temporarily\n",
    "                with open(\"temp_audio.wav\", \"wb\") as f:\n",
    "                    f.write(uploaded_audio.read())\n",
    "                \n",
    "                # Transcribe\n",
    "                result = whisper_model.transcribe(\"temp_audio.wav\")\n",
    "                transcription = result[\"text\"]\n",
    "                \n",
    "                st.write(f\"**Transcription:** {transcription}\")\n",
    "                \n",
    "                # Generate response\n",
    "                response = llm_pipeline(\n",
    "                    f\"User: {transcription}\\\\nAssistant:\",\n",
    "                    max_length=100,\n",
    "                    temperature=0.7\n",
    "                )[0]['generated_text']\n",
    "                \n",
    "                assistant_response = response.split(\"Assistant:\")[-1].strip()\n",
    "                st.write(f\"**Response:** {assistant_response}\")\n",
    "    \n",
    "    # Clear conversation\n",
    "    if st.button(\"Clear Conversation\"):\n",
    "        st.session_state.conversation = []\n",
    "        st.rerun()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save Streamlit app\n",
    "with open(\"voice_assistant_app.py\", \"w\") as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(\"Streamlit app saved as 'voice_assistant_app.py'\")\n",
    "print(\"To run: streamlit run voice_assistant_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a1064",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e102e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_voice_assistant():\n",
    "    \"\"\"Optimization tips and techniques for voice assistant.\"\"\"\n",
    "    \n",
    "    optimization_tips = \"\"\"\n",
    "    === VOICE ASSISTANT OPTIMIZATION ===\n",
    "    \n",
    "    1. MODEL OPTIMIZATION:\n",
    "       - Use smaller Whisper models (tiny, base) for faster inference\n",
    "       - Implement model quantization for reduced memory usage\n",
    "       - Cache models in memory to avoid reloading\n",
    "       - Use GPU acceleration when available\n",
    "    \n",
    "    2. AUDIO PROCESSING:\n",
    "       - Implement VAD (Voice Activity Detection) to reduce processing\n",
    "       - Use streaming audio processing for real-time responses\n",
    "       - Optimize audio sample rates (16kHz is sufficient for speech)\n",
    "       - Implement noise reduction preprocessing\n",
    "    \n",
    "    3. RESPONSE GENERATION:\n",
    "       - Use smaller, faster LLMs for quicker responses\n",
    "       - Implement response caching for common queries\n",
    "       - Limit response length to reduce TTS time\n",
    "       - Use async processing for multiple components\n",
    "    \n",
    "    4. TTS OPTIMIZATION:\n",
    "       - Pre-load Bark models to reduce initialization time\n",
    "       - Use shorter voice presets for faster generation\n",
    "       - Implement streaming TTS for long responses\n",
    "       - Cache common responses as audio files\n",
    "    \n",
    "    5. SYSTEM OPTIMIZATION:\n",
    "       - Use multithreading for parallel processing\n",
    "       - Implement proper error handling and recovery\n",
    "       - Monitor memory usage and implement cleanup\n",
    "       - Use connection pooling for API calls\n",
    "    \"\"\"\n",
    "    \n",
    "    return optimization_tips\n",
    "\n",
    "print(optimize_voice_assistant())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38364e36",
   "metadata": {},
   "source": [
    "## 8. Memory Cleanup and Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4659a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources():\n",
    "    \"\"\"Clean up memory and resources.\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    # Clear variables\n",
    "    if 'assistant' in globals():\n",
    "        del assistant\n",
    "    if 'advanced_assistant' in globals():\n",
    "        del advanced_assistant\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"Resources cleaned up\")\n",
    "\n",
    "# Clean up\n",
    "cleanup_resources()\n",
    "\n",
    "print(\"Voice Assistant notebook completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1344d0a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated a complete voice assistant implementation featuring:\n",
    "\n",
    "### Key Components:\n",
    "1. **Speech Recognition** - Whisper for accurate transcription\n",
    "2. **Language Understanding** - LLM for contextual responses\n",
    "3. **Speech Synthesis** - Bark for natural voice generation\n",
    "4. **Web Interface** - Streamlit for user interaction\n",
    "\n",
    "### Advanced Features:\n",
    "- Wake word detection\n",
    "- Intent recognition\n",
    "- Conversation history\n",
    "- Multi-modal input (text and audio)\n",
    "- Real-time processing\n",
    "\n",
    "### Applications:\n",
    "- **Personal Assistant**: Schedule management, reminders\n",
    "- **Customer Service**: Automated support systems\n",
    "- **Accessibility**: Voice-controlled interfaces\n",
    "- **Education**: Interactive learning companions\n",
    "- **Smart Home**: Voice-controlled IoT devices\n",
    "\n",
    "### Next Steps:\n",
    "1. **Integration**: Connect with external APIs (weather, calendar)\n",
    "2. **Personalization**: User-specific voice and preferences\n",
    "3. **Multilingual**: Support for multiple languages\n",
    "4. **Mobile**: Deploy on mobile platforms\n",
    "5. **IoT**: Integration with smart home devices\n",
    "\n",
    "**Note**: This implementation provides a foundation for voice assistant development. For production use, consider privacy, security, and performance optimizations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
